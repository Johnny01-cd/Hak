{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python numpy matplotlib scikit-learn imbalanced-learn keras \n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'extract_lfw'\n",
    "df_images = []\n",
    "labels = []  # List to store image labels\n",
    "for label, person in enumerate(os.listdir(folder_path)):\n",
    "    if os.path.isdir(os.path.join(folder_path, person)):\n",
    "        image_files = [filename for filename in os.listdir(os.path.join(folder_path, person)) if filename.endswith('.jpg')]\n",
    "        \n",
    "        # Check if there are images in the subfolder\n",
    "        if image_files:\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(folder_path, person, filename)\n",
    "                img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.normalize(img, None, 0, 1.0,cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                img = img.flatten()\n",
    "                df_images.append(img)\n",
    "                labels.append(label)  # Assigning label based on folder/class\n",
    "        else:\n",
    "            print(f\"Skipping '{person}' due to no images in the subfolder.\")\n",
    "df_images = np.array(df_images)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assuming 'X' is your data and 'y' is the corresponding labels\n",
    "df_images, labels = shuffle(df_images, labels , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 62500) (2115,)\n",
      "(907, 62500) (907,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_images, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setting df_images and labels to 0 to deallocate memory\n",
    "df_images = 0\n",
    "labels = 0\n",
    "\n",
    "# Printing the shapes of the training and testing sets\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23374, 62500)\n",
      "(23374,)\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.9 GiB for an array with shape (46748, 62500) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\OneDrive - American University of Phnom Penh\\Fall 2023\\ML\\Final_Project\\Week2.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming df_images, X_train, and X_test are defined previously\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Apply PCA to the scaled training data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m,svd_solver\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m,whiten\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_train_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(X_final)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Apply the same transformation to the testing data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/OneDrive%20-%20American%20University%20of%20Phnom%20Penh/Fall%202023/ML/Final_Project/Week2.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_test_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    461\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    464\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    478\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    484\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    485\u001b[0m )\n\u001b[0;32m    487\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:986\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 986\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(\n\u001b[0;32m    987\u001b[0m             array, dtype\u001b[39m=\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, xp\u001b[39m=\u001b[39;49mxp\n\u001b[0;32m    988\u001b[0m         )\n\u001b[0;32m    989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[39m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     array \u001b[39m=\u001b[39m _asarray_with_order(\n\u001b[0;32m    992\u001b[0m         array, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xp\u001b[39m=\u001b[39mxp\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:378\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    376\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.9 GiB for an array with shape (46748, 62500) and data type float32"
     ]
    }
   ],
   "source": [
    "# Assuming df_images, X_train, and X_test are defined previously\n",
    "\n",
    "# Apply PCA to the scaled training data\n",
    "pca = PCA(n_components=150,svd_solver=\"arpack\",whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_final)\n",
    "\n",
    "# Apply the same transformation to the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Plot explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(cumulative_var_ratio)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of transformed data after PCA\n",
    "print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "print(\"X_test_pca shape:\", X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Training Data')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Data Points in PCA Space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Svm = SVC(C= 1.4872681066575715, gamma= 0.003293320249656128, kernel= \"rbf\", class_weight=\"balanced\")\n",
    "Svm.fit(X_train_pca, y_resampled)\n",
    "\n",
    "\n",
    "y_pred =Svm.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": loguniform(1e-1, 1e3),  # Adjusted range for 'C'\n",
    "    \"gamma\": loguniform(1e-5, 1e-1),  # Adjusted range for 'gamma'\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid','linear']\n",
    "}\n",
    "\n",
    "# Use a scoring metric, such as accuracy\n",
    "clf = RandomizedSearchCV(\n",
    "    SVC(class_weight=\"balanced\"), param_grid, cv=10, scoring='accuracy', n_iter=100\n",
    ")\n",
    "\n",
    "clf.fit(X_test_pca, y_resampled)\n",
    "best_params = clf.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
