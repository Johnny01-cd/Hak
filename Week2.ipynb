{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python numpy matplotlib scikit-learn imbalanced-learn keras \n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'extract_lfw'\n",
    "df_images = []\n",
    "labels = []  # List to store image labels\n",
    "for label, person in enumerate(os.listdir(folder_path)):\n",
    "    if os.path.isdir(os.path.join(folder_path, person)):\n",
    "        image_files = [filename for filename in os.listdir(os.path.join(folder_path, person)) if filename.endswith('.jpg')]\n",
    "        \n",
    "        # Check if there are images in the subfolder\n",
    "        if image_files:\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(folder_path, person, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.normalize(img, None, 0, 1.0,cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                img = img.flatten()\n",
    "                df_images.append(img)\n",
    "                labels.append(label)  # Assigning label based on folder/class\n",
    "        else:\n",
    "            print(f\"Skipping '{person}' due to no images in the subfolder.\")\n",
    "df_images = np.array(df_images)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assuming 'X' is your data and 'y' is the corresponding labels\n",
    "df_images, labels = shuffle(df_images, labels , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 62500) (2115,)\n",
      "(907, 62500) (907,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_images, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setting df_images and labels to 0 to deallocate memory\n",
    "df_images = 0\n",
    "labels = 0\n",
    "\n",
    "# Printing the shapes of the training and testing sets\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23374, 62500)\n",
      "(23374,)\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
   "source": [
    "# Assuming df_images, X_train, and X_test are defined previously\n",
    "\n",
    "# Apply PCA to the scaled training data\n",
    "pca = PCA(n_components=150,svd_solver=\"arpack\",whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_final)\n",
    "\n",
    "# Apply the same transformation to the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Plot explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(cumulative_var_ratio)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of transformed data after PCA\n",
    "print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "print(\"X_test_pca shape:\", X_test_pca.shape)"
   ]
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Training Data')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Data Points in PCA Space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of components\n",
    "n_classes = len(np.unique(y_resampled))  # Calculate the number of unique classes\n",
    "n_features = X_resampled.shape[1]  # Number of features in the resampled data\n",
    "\n",
    "# Determine the number of components using the formula\n",
    "n_components = min(n_classes - 1, n_features)\n",
    "\n",
    "# Use LDA with the calculated number of components\n",
    "LdaScale = LDA(n_components=n_components)\n",
    "X_train_lda = LdaScale.fit_transform(X_resampled, y_resampled)\n",
    "X_test_lda = LdaScale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot in the LDA-transformed space\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for label in np.unique(y_train):\n",
    "    plt.scatter(X_train_lda[y_train == label, 0], X_train_lda[y_train == label, 1],\n",
    "                label=f'Class {label}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.legend()\n",
    "plt.title('Scatter Plot of LDA-transformed Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Svm = SVC(C= 1.4872681066575715, gamma= 0.003293320249656128, kernel= \"rbf\", class_weight=\"balanced\")\n",
    "Svm.fit(X_train_pca, y_resampled)\n",
    "\n",
    "\n",
    "y_pred =Svm.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": loguniform(1e-1, 1e3),  # Adjusted range for 'C'\n",
    "    \"gamma\": loguniform(1e-5, 1e-1),  # Adjusted range for 'gamma'\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid','linear']\n",
    "}\n",
    "\n",
    "# Use a scoring metric, such as accuracy\n",
    "clf = RandomizedSearchCV(\n",
    "    SVC(class_weight=\"balanced\"), param_grid, cv=10, scoring='accuracy', n_iter=100\n",
    ")\n",
    "\n",
    "clf.fit(X_train_lda, y_resampled)\n",
    "best_params = clf.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = clf.predict(X_test_lda)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
